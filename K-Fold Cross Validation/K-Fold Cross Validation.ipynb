{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation\n",
    "\n",
    "## Mathematical Definition of K-Fold Cross Validation\n",
    "\n",
    "K-Fold Cross Validation is a widely used resampling procedure to evaluate machine learning models on a limited data sample. The general procedure is as follows:\n",
    "\n",
    "1.  **Shuffle the Dataset**: Randomly shuffle the dataset to ensure that each fold is a good representative of the overall data distribution.\n",
    "\n",
    "2.  **Split into K Folds**: Divide the shuffled dataset into $K$ equally sized (or as equally sized as possible) folds or subsets. Let the total number of samples in the dataset be $N$. Each fold will contain approximately $N/K$ samples.\n",
    "\n",
    "3.  **Iterate K Times**: For each of the $K$ folds, the following steps are performed:\n",
    "    a.  **Training Set**: One fold is used as the validation (or test) set.\n",
    "    b.  **Validation Set**: The remaining $K-1$ folds are combined to form the training set.\n",
    "    c.  **Model Training**: A machine learning model is trained on the training set.\n",
    "    d.  **Model Evaluation**: The trained model is evaluated on the validation set, and a performance metric (e.g., accuracy, precision, recall, F1-score, MSE) is recorded.\n",
    "\n",
    "4.  **Aggregate Results**: After $K$ iterations, $K$ different performance metrics are obtained. The final performance of the model is typically the average of these $K$ metrics.\n",
    "\n",
    "### Mathematical Notation\n",
    "Let $D = \\{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\\}$ be the entire dataset with $N$ samples.\n",
    "\n",
    "The dataset $D$ is partitioned into $K$ disjoint subsets (folds):\n",
    "\n",
    "$$D = D_1 \\cup D_2 \\cup ... \\cup D_K \\quad \\text{where} \\quad D_i \\cap D_j = \\emptyset \\text{ for } i \\neq j$$\n",
    "\n",
    "For each iteration $k \\in \\{1, ..., K\\}\\$:\n",
    "\n",
    "-   **Validation Set**: $D_{test}^{(k)} = D_k$\\n\n",
    "-   **Training Set**: $D_{train}^{(k)} = D \\setminus D_k = \\bigcup_{j=1, j \\neq k}^{K} D_j$\\n\n",
    "\n",
    "Let $M$ be a machine learning model with parameters $\\theta$. In each iteration $k$, the model is trained on $D_{train}^{(k)}$ to obtain parameters $\\hat{\\theta}^{(k)}$:\n",
    "\n",
    "$$M_k = \\text{train}(D_{train}^{(k)})$$ \n",
    "\n",
    "The performance of $M_k$ is evaluated on $D_{test}^{(k)}$ using a chosen metric $E$. Let $E(M_k, D_{test}^{(k)})$ denote this performance.\n",
    "\n",
    "The final estimated performance of the model is the average of the $K$ individual performances:\n",
    "\n",
    "$$\\text{Estimated Performance} = \\frac{1}{K} \\sum_{k=1}^{K} E(M_k, D_{test}^{(k)})$$ \n",
    "\n",
    "### Advantages\n",
    "-   **Reduced Bias**: All data points are used for both training and validation, reducing the bias of the performance estimate.\n",
    "-   **Reduced Variance**: The variance of the performance estimate is reduced compared to a single train-test split, as it averages results over multiple splits.\n",
    "-   **Efficient Data Usage**: Particularly useful when the dataset is small, as it makes maximum use of the available data.\n",
    "\n",
    "### Disadvantages\n",
    "-   **Computationally Expensive**: Training the model $K$ times can be computationally intensive, especially for large datasets or complex models.\n",
    "-   **Not Suitable for Time Series**: The random shuffling step makes it unsuitable for time series data where the temporal order is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=10000)\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9861111111111112"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9722222222222222"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=40, min_samples_split=2, max_depth=10)\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Score: 0.9666110183639399\n",
      "SVM Score: 0.986644407345576\n",
      "Random Forest Score: 0.9682804674457429\n",
      "------------------------------\n",
      "Logistic Regression Score: 0.9649415692821369\n",
      "SVM Score: 0.988313856427379\n",
      "Random Forest Score: 0.9616026711185309\n",
      "------------------------------\n",
      "Logistic Regression Score: 0.9515859766277128\n",
      "SVM Score: 0.986644407345576\n",
      "Random Forest Score: 0.9766277128547579\n",
      "------------------------------\n",
      "Average Logistic Regression Score: 0.9610461880912632\n",
      "Average SVM Score: 0.9872008903728436\n",
      "Average Random Forest Score: 0.9688369504730105\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "splits= 3\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "lrscore = 0\n",
    "svmscore = 0\n",
    "rfscore = 0\n",
    "\n",
    "for train_index, test_index in kf.split(digits.data):\n",
    "    X_train_kf, X_test_kf = digits.data[train_index], digits.data[test_index]\n",
    "    y_train_kf, y_test_kf = digits.target[train_index], digits.target[test_index]\n",
    "    \n",
    "    lr.fit(X_train_kf, y_train_kf)\n",
    "    print(\"Logistic Regression Score:\", lr.score(X_test_kf, y_test_kf))\n",
    "    lrscore += lr.score(X_test_kf, y_test_kf)\n",
    "    \n",
    "    svm.fit(X_train_kf, y_train_kf)\n",
    "    print(\"SVM Score:\", svm.score(X_test_kf, y_test_kf))\n",
    "    svmscore += svm.score(X_test_kf, y_test_kf)\n",
    "\n",
    "    rf.fit(X_train_kf, y_train_kf)\n",
    "    print(\"Random Forest Score:\", rf.score(X_test_kf, y_test_kf))\n",
    "    rfscore += rf.score(X_test_kf, y_test_kf)\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"Average Logistic Regression Score:\", lrscore / splits)\n",
    "print(\"Average SVM Score:\", svmscore / splits)\n",
    "print(\"Average Random Forest Score:\", rfscore / splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified Logistic Regression Score: 0.9616026711185309\n",
      "Stratified SVM Score: 0.986644407345576\n",
      "Stratified Random Forest Score: 0.9749582637729549\n",
      "------------------------------\n",
      "Stratified Logistic Regression Score: 0.9782971619365609\n",
      "Stratified SVM Score: 0.991652754590985\n",
      "Stratified Random Forest Score: 0.9699499165275459\n",
      "------------------------------\n",
      "Stratified Logistic Regression Score: 0.9632721202003339\n",
      "Stratified SVM Score: 0.9833055091819699\n",
      "Stratified Random Forest Score: 0.9782971619365609\n",
      "------------------------------\n",
      "Average Stratified Logistic Regression Score: 0.988313856427379\n",
      "Average Stratified SVM Score: 0.994991652754591\n",
      "Average Stratified Random Forest Score: 0.991652754590985\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "splits= 3\n",
    "lrscore = 0\n",
    "svmscore = 0\n",
    "rfscore = 0\n",
    "\n",
    "folds = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in folds.split(digits.data, digits.target):\n",
    "    X_train_skf, X_test_skf = digits.data[train_index], digits.data[test_index]\n",
    "    y_train_skf, y_test_skf = digits.target[train_index], digits.target[test_index]\n",
    "    \n",
    "    lr.fit(X_train_skf, y_train_skf)\n",
    "    print(\"Stratified Logistic Regression Score:\", lr.score(X_test_skf, y_test_skf))\n",
    "    lrscore += lr.score(X_test_kf, y_test_kf)\n",
    "\n",
    "\n",
    "    svm.fit(X_train_skf, y_train_skf)\n",
    "    print(\"Stratified SVM Score:\", svm.score(X_test_skf, y_test_skf))\n",
    "    svmscore += svm.score(X_test_kf, y_test_kf)\n",
    "\n",
    "\n",
    "    rf.fit(X_train_skf, y_train_skf)\n",
    "    print(\"Stratified Random Forest Score:\", rf.score(X_test_skf, y_test_skf))\n",
    "    rfscore += rf.score(X_test_kf, y_test_kf)\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"Average Stratified Logistic Regression Score:\", lrscore / splits)\n",
    "print(\"Average Stratified SVM Score:\", svmscore / splits)\n",
    "print(\"Average Stratified Random Forest Score:\", rfscore / splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences between K-Fold Cross Validation and Stratified K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Process: The dataset is randomly divided into k equal-sized\n",
    "folds. In each iteration, one fold is used as the test set,\n",
    "and the remaining k-1 folds are used as the training set.\n",
    "\n",
    "* Class Distribution: It does not guarantee that each fold will\n",
    "  have the same proportion of class labels as the original\n",
    "  dataset. If the dataset has an imbalanced class distribution,\n",
    "  some folds might end up with a disproportionate number of\n",
    "  samples from a particular class, leading to biased model\n",
    "  evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Process: This is a variation of K-Fold Cross Validation that\n",
    "ensures each fold maintains the same proportion of class\n",
    "labels as the original dataset. The data is divided into k\n",
    "folds such that the percentage of samples for each class is\n",
    "preserved in each fold.\n",
    "\n",
    "* Class Distribution: It is particularly useful when dealing\n",
    "with imbalanced datasets, as it prevents any single fold from\n",
    "having a significantly different class distribution, thus\n",
    "providing a more reliable and less biased estimate of the\n",
    "model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Difference Summary:\n",
    "\n",
    "\n",
    "* K-Fold: Randomly splits data, no guarantee of class proportion in\n",
    "folds.\n",
    "\n",
    "* Stratified K-Fold: Preserves the percentage of samples for each\n",
    "class in each fold, making it suitable for imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9259877573734001"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "np.mean(cross_val_score(LogisticRegression(max_iter=10000), digits.data, digits.target, cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9699499165275459"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(SVC(), digits.data, digits.target, cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9343350027824151"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(RandomForestClassifier(n_estimators=40, min_samples_split=2, max_depth=10), digits.data, digits.target, cv=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
