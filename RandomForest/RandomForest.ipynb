{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d3e4f5",
   "metadata": {},
   "source": [
    "# Random Forest from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4d5e6",
   "metadata": {},
   "source": [
    "## 1. Introduction to Random Forest\n",
    "\n",
    "Random Forest is an ensemble learning method for classification and regression that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "\n",
    "It addresses the problem of overfitting that individual decision trees often suffer from, and it generally provides higher accuracy and stability.\n",
    "\n",
    "Key concepts that make Random Forest powerful:\n",
    "- **Ensemble Learning**: Combining multiple models to improve overall performance.\n",
    "- **Decision Trees**: The fundamental building blocks of a Random Forest.\n",
    "- **Bagging (Bootstrap Aggregating)**: A technique to reduce variance by training multiple models on different subsets of the training data.\n",
    "- **Random Feature Subspace**: Introducing randomness in feature selection when building individual trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d5e6f7",
   "metadata": {},
   "source": [
    "## 2. Decision Trees (Building Blocks)\n",
    "\n",
    "A decision tree is a flowchart-like structure where each internal node represents a \"test\" on an attribute (e.g., whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Root Node**: The topmost node in the tree, representing the entire dataset.\n",
    "- **Internal Node**: A node that has one or more branches, representing a decision based on a feature.\n",
    "- **Leaf Node (Terminal Node)**: A node that does not have any branches, representing the final outcome or prediction.\n",
    "- **Splitting**: The process of dividing a node into two or more sub-nodes based on a splitting criterion.\n",
    "- **Pruning**: The process of removing branches from the tree to prevent overfitting.\n",
    "\n",
    "**How a Decision Tree is Built (Simplified):**\n",
    "1.  Start with the entire dataset at the root node.\n",
    "2.  Select the best attribute to split the data based on a criterion (e.g., Gini impurity, entropy for classification; Mean Squared Error for regression).\n",
    "3.  Divide the dataset into subsets based on the values of the chosen attribute.\n",
    "4.  Recursively repeat steps 2 and 3 for each subset until a stopping condition is met (e.g., all samples in a node belong to the same class, maximum depth is reached, or minimum number of samples per leaf is met).\n",
    "\n",
    "**Mathematical Intuition (for Classification - Gini Impurity):**\n",
    "\n",
    "Gini Impurity measures the impurity of a node. A node is pure if all its samples belong to the same class (Gini impurity = 0). The goal is to minimize Gini impurity at each split.\n",
    "\n",
    "$Gini(D) = 1 - \\sum_{i=1}^{c} (p_i)^2$\n",
    "\n",
    "Where:\n",
    "- $D$ is the dataset (or a node).\n",
    "- $c$ is the number of classes.\n",
    "- $p_i$ is the proportion of samples belonging to class $i$ in the dataset $D$.\n",
    "\n",
    "When splitting a node into two child nodes (left and right), the weighted average Gini impurity is calculated:\n",
    "\n",
    "$Gini_{split} = \\frac{N_{left}}{N} Gini(D_{left}) + \\frac{N_{right}}{N} Gini(D_{right})$\n",
    "\n",
    "Where:\n",
    "- $N_{left}$ and $N_{right}$ are the number of samples in the left and right child nodes, respectively.\n",
    "- $N$ is the total number of samples in the parent node.\n",
    "\n",
    "The attribute that results in the lowest $Gini_{split}$ (or highest information gain) is chosen for the split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6f7a8",
   "metadata": {},
   "source": [
    "## 3. Bagging (Bootstrap Aggregating)\n",
    "\n",
    "Bagging, short for Bootstrap Aggregating, is an ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms. It also helps to reduce variance and avoid overfitting.\n",
    "\n",
    "**How it works:**\n",
    "1.  **Bootstrap Sampling**: From the original training dataset, multiple subsets are created by sampling with replacement. This means that some data points may appear multiple times in a subset, while others may not appear at all. Each subset is of the same size as the original dataset.\n",
    "2.  **Model Training**: An independent model (e.g., a decision tree) is trained on each of these bootstrap samples.\n",
    "3.  **Aggregation**: For classification tasks, the final prediction is made by taking a majority vote of the predictions from all individual models. For regression tasks, the final prediction is the average of the predictions from all individual models.\n",
    "\n",
    "**Benefits of Bagging:**\n",
    "-   **Reduces Variance**: By averaging or voting across multiple models trained on different data subsets, the impact of noisy data or outliers on any single model is reduced.\n",
    "-   **Improves Stability**: The overall model becomes more robust to small changes in the training data.\n",
    "-   **Handles Overfitting**: By introducing randomness in the data sampling, it helps to prevent individual models from overfitting to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7a8b9",
   "metadata": {},
   "source": [
    "## 4. Random Feature Subset Selection\n",
    "\n",
    "While bagging helps reduce variance by sampling data, Random Forest introduces an additional layer of randomness: **random feature subset selection**.\n",
    "\n",
    "**How it works:**\n",
    "When building each individual decision tree in the forest, at each split point, instead of considering all available features, only a random subset of features is considered. This means that even if there's a very strong predictor feature, not all trees will be built using it, forcing them to explore other features.\n",
    "\n",
    "**Benefits:**\n",
    "-   **Decorrelates Trees**: This is the most crucial benefit. If you have one or a few very strong predictor features, these features would be chosen at the top of almost every tree in a standard bagging approach, leading to highly correlated trees. Averaging correlated trees doesn't reduce variance as much as averaging uncorrelated trees. By randomly selecting a subset of features, the trees become more diverse and less correlated.\n",
    "-   **Reduces Overfitting**: By limiting the features available at each split, it further reduces the chance of individual trees overfitting to specific features in the training data.\n",
    "-   **Computational Efficiency**: For datasets with a very large number of features, considering only a subset of features at each split can speed up the tree building process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8b9c0",
   "metadata": {},
   "source": [
    "## 5. Algorithm Steps\n",
    "\n",
    "Here's a step-by-step breakdown of how the Random Forest algorithm works:\n",
    "\n",
    "1.  **Input**: A training dataset $D = \\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\}$ and the number of trees to build $T$.\n",
    "\n",
    "2.  **For each tree $t$ from $1$ to $T$:**\n",
    "    a.  **Bootstrap Sampling**: Create a bootstrap sample $D_t$ by randomly sampling $n$ data points from $D$ with replacement. This $D_t$ will be the training data for tree $t$.\n",
    "    b.  **Tree Construction**: Grow a decision tree $C_t$ from the bootstrap sample $D_t$. During the tree growing process, at each split:\n",
    "        i.  **Random Feature Subset Selection**: Randomly select a subset of $m$ features from the total $M$ available features ($m \\ll M$).\n",
    "        ii. **Best Split**: Find the best split among these $m$ features using a splitting criterion (e.g., Gini impurity for classification, Mean Squared Error for regression).\n",
    "        iii. **Split Node**: Split the node into two child nodes based on the best split.\n",
    "        iv. **Repeat**: Recursively repeat steps i-iii until a stopping condition is met (e.g., maximum depth, minimum samples per leaf, or no further gain from splitting).\n",
    "\n",
    "3.  **Output**: The ensemble of $T$ decision trees ${C_1, C_2, ..., C_T}$.\n",
    "\n",
    "**Prediction Phase:**\n",
    "\n",
    "To make a prediction for a new, unseen data point $x_{new}$:\n",
    "\n",
    "1.  **Individual Predictions**: Pass $x_{new}$ through each of the $T$ decision trees to get $T$ individual predictions: $P_1, P_2, ..., P_T$.\n",
    "\n",
    "2.  **Aggregation**:\n",
    "    a.  **For Classification**: The final prediction is the class that receives the majority vote among all $T$ trees.\n",
    "    b.  **For Regression**: The final prediction is the average of the predictions from all $T$ trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b9c0d1-1",
   "metadata": {},
   "source": [
    "## 6. Implementation from Scratch\n",
    "\n",
    "We will implement a simplified version of Random Forest from scratch. This will involve creating a `DecisionTree` class and then a `RandomForest` class that utilizes multiple `DecisionTree` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8b9c0d1-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value # For leaf nodes\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.root = None\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        most_common = counter.most_common(1)[0][0]\n",
    "        return most_common\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # Check stopping criteria\n",
    "        if (depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
    "\n",
    "        best_feature, best_threshold = self._best_split(X, y, feat_idxs)\n",
    "\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_threshold)\n",
    "        left_child = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right_child = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "        return Node(best_feature, best_threshold, left_child, right_child)\n",
    "\n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(y, X_column, threshold)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = threshold\n",
    "\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "\n",
    "        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum([p * np.log(p) for p in ps if p > 0])\n",
    "\n",
    "    def _split(self, X_column, threshold):\n",
    "        left_idxs = np.argwhere(X_column <= threshold).flatten()\n",
    "        right_idxs = np.argwhere(X_column > threshold).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8b9c0d1-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, n_trees=10, min_samples_split=2, max_depth=100, n_features=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            tree = DecisionTree(\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                max_depth=self.max_depth,\n",
    "                n_features=self.n_features,\n",
    "            )\n",
    "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        return X[idxs], y[idxs]\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        # Transpose to have predictions for each sample in columns\n",
    "        tree_preds = np.swapaxes(predictions, 0, 1)\n",
    "        # Get majority vote for each sample\n",
    "        y_pred = np.array([self._most_common_label(pred) for pred in tree_preds])\n",
    "        return y_pred\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        most_common = counter.most_common(1)[0][0]\n",
    "        return most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c0d1e2-1",
   "metadata": {},
   "source": [
    "## 7. Example Usage\n",
    "\n",
    "Let's test our Random Forest implementation with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c0d1e2-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "data = datasets.load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Initialize and train the Random Forest\n",
    "# You can adjust n_trees, max_depth, and n_features\n",
    "clf = RandomForest(n_trees=20, max_depth=10, n_features=5)\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "print(f\"Random Forest Accuracy: {accuracy(y_test, predictions)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
