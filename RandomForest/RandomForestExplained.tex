\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{hyperref}

\geometry{margin=1in}

% Python code styling
\lstset{
    language=Python,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue}
\bfseries,
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    showstringspaces=false,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray}
}

\title{Random Forests: Mathematical Foundations and Implementation}
\author{Machine Learning Tutorial}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Introduction to Random Forests}

Random Forest is an ensemble learning method for classification and regression that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. It addresses the problem of overfitting that individual decision trees often suffer from, and it generally provides higher accuracy and stability.

Key concepts that make Random Forest powerful:
\begin{itemize}
    \item \textbf{Ensemble Learning}: Combining multiple models to improve overall performance.
    \item \textbf{Decision Trees}: The fundamental building blocks of a Random Forest.
    \item \textbf{Bagging (Bootstrap Aggregating)}: A technique to reduce variance by training multiple models on different subsets of the training data.
    \item \textbf{Random Feature Subspace}: Introducing randomness in feature selection when building individual trees.
\end{itemize}

\section{Core Concepts}

\subsection{Decision Trees (Building Blocks)}

A decision tree is a flowchart-like structure where each internal node represents a \"test\" on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label. The quality of a split is often measured using Gini impurity:

\begin{equation}
\text{Gini}(D) = 1 - \sum_{i=1}^{c} (p_i)^2
\end{equation}

Where $p_i$ is the proportion of samples belonging to class $i$ in the dataset $D$.

\subsection{Bagging (Bootstrap Aggregating)}

Bagging, short for Bootstrap Aggregating, is an ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms. It also helps to reduce variance and avoid overfitting.

\textbf{How it works:}
\begin{enumerate}
    \item \textbf{Bootstrap Sampling}: From the original training dataset, multiple subsets are created by sampling with replacement. Each subset is of the same size as the original dataset.
    \item \textbf{Model Training}: An independent model (e.g., a decision tree) is trained on each of these bootstrap samples.
    \item \textbf{Aggregation}: For classification tasks, the final prediction is made by taking a majority vote of the predictions from all individual models. For regression tasks, the final prediction is the average of the predictions.
\end{enumerate}

\subsection{Random Feature Subset Selection}

While bagging helps reduce variance by sampling data, Random Forest introduces an additional layer of randomness: random feature subset selection. When building each individual decision tree in the forest, at each split point, instead of considering all available features, only a random subset of features is considered. This decorrelates the trees and further reduces overfitting.

\section{The Random Forest Algorithm}

The Random Forest algorithm combines these concepts:

\begin{algorithm}
\caption{Random Forest Construction}
\begin{algorithmic}[1]
\STATE \textbf{Input}: A training dataset $D = \{(x_1, y_1), ..., (x_n, y_n)\}$, number of trees $T$.
\FOR{$t=1$ to $T$}
    \STATE Create a bootstrap sample $D_t$ by sampling $n$ data points from $D$ with replacement.
    \STATE Grow a decision tree $C_t$ from $D_t$. At each node, during splitting:
    \STATE Randomly select a subset of $m$ features from the total $M$ available features ($m \ll M$).
    \STATE Find the best split among the subset of $m$ features.
\ENDFOR
\STATE \textbf{Output}: The ensemble of trees ${C_1, ..., C_T}$.
\end{algorithmic}
\end{algorithm}

\section{Implementation from Scratch}

\subsection{Node and DecisionTree Classes}
First, we need a `Node` class and a `DecisionTree` class. The `DecisionTree` is built recursively, using information gain (or Gini impurity) to find the best split.

\begin{lstlisting}
import numpy as np
from collections import Counter

class Node:
    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):
        self.feature = feature
        self.threshold = threshold
        self.left = left
        self.right = right
        self.value = value # For leaf nodes

    def is_leaf_node(self):
        return self.value is not None

class DecisionTree:
    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):
        self.min_samples_split = min_samples_split
        self.max_depth = max_depth
        self.n_features = n_features
        self.root = None

    def _most_common_label(self, y):
        counter = Counter(y)
        most_common = counter.most_common(1)[0][0]
        return most_common

    def fit(self, X, y):
        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)
        self.root = self._grow_tree(X, y)

    def _grow_tree(self, X, y, depth=0):
        n_samples, n_feats = X.shape
        n_labels = len(np.unique(y))

        # Check stopping criteria
        if (depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):
            leaf_value = self._most_common_label(y)
            return Node(value=leaf_value)

        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)

        best_feature, best_threshold = self._best_split(X, y, feat_idxs)

        left_idxs, right_idxs = self._split(X[:, best_feature], best_threshold)
        left_child = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)
        right_child = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)
        return Node(best_feature, best_threshold, left_child, right_child)

    def _best_split(self, X, y, feat_idxs):
        best_gain = -1
        split_idx, split_threshold = None, None

        for feat_idx in feat_idxs:
            X_column = X[:, feat_idx]
            thresholds = np.unique(X_column)

            for threshold in thresholds:
                gain = self._information_gain(y, X_column, threshold)

                if gain > best_gain:
                    best_gain = gain
                    split_idx = feat_idx
                    split_threshold = threshold

        return split_idx, split_threshold

    def _information_gain(self, y, X_column, threshold):
        parent_entropy = self._entropy(y)

        left_idxs, right_idxs = self._split(X_column, threshold)

        if len(left_idxs) == 0 or len(right_idxs) == 0:
            return 0

        n = len(y)
        n_l, n_r = len(left_idxs), len(right_idxs)
        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])

        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r

        information_gain = parent_entropy - child_entropy
        return information_gain

    def _entropy(self, y):
        hist = np.bincount(y)
        ps = hist / len(y)
        return -np.sum([p * np.log(p) for p in ps if p > 0])

    def _split(self, X_column, threshold):
        left_idxs = np.argwhere(X_column <= threshold).flatten()
        right_idxs = np.argwhere(X_column > threshold).flatten()
        return left_idxs, right_idxs

    def predict(self, X):
        return np.array([self._traverse_tree(x, self.root) for x in X])

    def _traverse_tree(self, x, node):
        if node.is_leaf_node():
            return node.value

        if x[node.feature] <= node.threshold:
            return self._traverse_tree(x, node.left)
        return self._traverse_tree(x, node.right)
\end{lstlisting}

\subsection{RandomForest Class}
The `RandomForest` class orchestrates the creation of multiple decision trees.

\begin{lstlisting}
class RandomForest:
    def __init__(self, n_trees=10, min_samples_split=2, max_depth=100, n_features=None):
        self.n_trees = n_trees
        self.min_samples_split = min_samples_split
        self.max_depth = max_depth
        self.n_features = n_features
        self.trees = []

    def fit(self, X, y):
        self.trees = []
        for _ in range(self.n_trees):
            tree = DecisionTree(
                min_samples_split=self.min_samples_split,
                max_depth=self.max_depth,
                n_features=self.n_features,
            )
            X_sample, y_sample = self._bootstrap_sample(X, y)
            tree.fit(X_sample, y_sample)
            self.trees.append(tree)

    def _bootstrap_sample(self, X, y):
        n_samples = X.shape[0]
        idxs = np.random.choice(n_samples, n_samples, replace=True)
        return X[idxs], y[idxs]

    def predict(self, X):
        predictions = np.array([tree.predict(X) for tree in self.trees])
        # Transpose to have predictions for each sample in columns
        tree_preds = np.swapaxes(predictions, 0, 1)
        # Get majority vote for each sample
        y_pred = np.array([self._most_common_label(pred) for pred in tree_preds])
        return y_pred

    def _most_common_label(self, y):
        counter = Counter(y)
        most_common = counter.most_common(1)[0][0]
        return most_common
\end{lstlisting}

\section{Usage Example}

\begin{lstlisting}
from sklearn import datasets
from sklearn.model_selection import train_test_split

def accuracy(y_true, y_pred):
    accuracy = np.sum(y_true == y_pred) / len(y_true)
    return accuracy

data = datasets.load_breast_cancer()
X, y = data.data, data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

# Initialize and train the Random Forest
clf = RandomForest(n_trees=20, max_depth=10, n_features=5)
clf.fit(X_train, y_train)
predictions = clf.predict(X_test)

print(f"Random Forest Accuracy: {accuracy(y_test, predictions)}")
\end{lstlisting}

\end{document}
