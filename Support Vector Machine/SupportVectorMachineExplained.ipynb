{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b487705",
   "metadata": {},
   "source": [
    "### Support Vector Machines explained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a91d42",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVMs) are powerful supervised learning models used for classification and\n",
    "  regression tasks. They are particularly effective in high-dimensional spaces and cases where the number of\n",
    "   dimensions is greater than the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1b6b21",
   "metadata": {},
   "source": [
    "### The goal : Finding the Optimal Hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bcdc7c",
   "metadata": {},
   "source": [
    "The primary goal of an SVM is to find an optimal hyperplane that best separates the data points of\n",
    "  different classes in a high-dimensional space. For a binary classification problem, if the data is\n",
    "  linearly separable, there can be infinitely many hyperplanes that separate the classes. SVM aims to find\n",
    "  the one that maximizes the margin between the closest data points of different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1650b44",
   "metadata": {},
   "source": [
    "#### Hyperplane Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7052f26d",
   "metadata": {},
   "source": [
    "In an n-dimensional space, a hyperplane can be defined by the equation:\n",
    "\n",
    "  $$w \\cdot x - b = 0$$\n",
    "\n",
    "\n",
    "  Where:\n",
    "   - $w$ is the weight vector (normal to the hyperplane).\n",
    "   - $x$ is the input data point vector.\n",
    "   - $b$ is the bias (or intercept) term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c08f7f6",
   "metadata": {},
   "source": [
    "#### The Margin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0380b14",
   "metadata": {},
   "source": [
    "The margin is the distance between the hyperplane and the closest data points from each class. These\n",
    "  closest data points are called support vectors. SVM seeks to maximize this margin.\n",
    "\n",
    "  For a data point $(x_i, y_i)$, where $y_i \\in \\{-1, 1\\}$ is the class label:\n",
    "\n",
    "\n",
    "   - If $w \\cdot x_i - b \\ge 1$ for $y_i = 1$\n",
    "   - If $w \\cdot x_i - b \\le -1$ for $y_i = -1$\n",
    "\n",
    "  These can be combined into a single inequality:\n",
    "\n",
    "  $$y_i (w \\cdot x_i - b) \\ge 1$$\n",
    "\n",
    "\n",
    "  The distance from a point $x_i$ to the hyperplane is given by:\n",
    "\n",
    "  $$\\text{distance} = \\frac{|w \\cdot x_i - b|}{||w||}$$\n",
    "\n",
    "\n",
    "  The support vectors lie on the hyperplanes $w \\cdot x - b = 1$ and $w \\cdot x - b = -1$. The distance\n",
    "  between these two hyperplanes (the margin) is:\n",
    "\n",
    "  $$\\text{Margin} = \\frac{2}{||w||}$$\n",
    "\n",
    "  To maximize the margin, we need to minimize $||w||$, which is equivalent to minimizing\n",
    "  $\\frac{1}{2}||w||^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628fd574",
   "metadata": {},
   "source": [
    "#### Optimization Problem (Hard Margin SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f305dd5e",
   "metadata": {},
   "source": [
    " For linearly separable data, the optimization problem is to minimize $\\frac{1}{2}||w||^2$ subject to the\n",
    "  constraint $y_i (w \\cdot x_i - b) \\ge 1$ for all $i$.\n",
    "\n",
    "  This is a convex optimization problem that can be solved using Lagrange multipliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f35b60",
   "metadata": {},
   "source": [
    "####  Soft Margin SVM (Handling Non-linearly Separable Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3b0f41",
   "metadata": {},
   "source": [
    "In most real-world scenarios, data is not perfectly linearly separable. To handle this, Soft Margin SVM\n",
    "  introduces slack variables ($\\xi_i \\ge 0$) and a regularization parameter (C).\n",
    "\n",
    "  The constraints become:\n",
    "\n",
    "  $$y_i (w \\cdot x_i - b) \\ge 1 - \\xi_i$$\n",
    "\n",
    "  The objective function is modified to:\n",
    "\n",
    "  $$\\text{minimize} \\quad \\frac{1}{2}||w||^2 + C \\sum_{i=1}^{n} \\xi_i$$\n",
    "\n",
    "\n",
    "  Where:\n",
    "   - $C$ is a hyperparameter that controls the trade-off between maximizing the margin and minimizing the\n",
    "     classification error. A small $C$ creates a larger margin but allows more misclassifications, while a\n",
    "     large $C$ creates a smaller margin but fewer misclassifications.\n",
    "   - $\\xi_i$ (xi) are the slack variables, representing the degree of misclassification of data point $x_i$.\n",
    "     If $\\xi_i = 0$, the point is correctly classified and outside the margin. If $0 < \\xi_i < 1$, the point\n",
    "     is correctly classified but within the margin. If $\\xi_i \\ge 1$, the point is misclassified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9305c781",
   "metadata": {},
   "source": [
    "#### The Kernel Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1577248",
   "metadata": {},
   "source": [
    "SVMs can effectively perform non-linear classification using the kernel trick. This involves mapping the\n",
    "  input data into a higher-dimensional feature space where it might become linearly separable. Instead of\n",
    "  explicitly transforming the data, kernel functions calculate the dot product of the transformed vectors in\n",
    "   the higher-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67fe2ea",
   "metadata": {},
   "source": [
    "Common kernel functions include:\n",
    "   - Linear Kernel: $K(x_i, x_j) = x_i \\cdot x_j$\n",
    "   - Polynomial Kernel: $K(x_i, x_j) = (\\gamma x_i \\cdot x_j + r)^d$\n",
    "   - Radial Basis Function (RBF) / Gaussian Kernel: $K(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdc2dbe",
   "metadata": {},
   "source": [
    "### Functioning (Training and Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbb77a3",
   "metadata": {},
   "source": [
    "#### Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75328d2c",
   "metadata": {},
   "source": [
    "1. Data Preparation: Prepare your labeled training data $(X, y)$.\n",
    "\n",
    "2. Feature Scaling: It's often beneficial to scale your features, especially for RBF kernels, as SVM is\n",
    "      sensitive to the magnitude of features.\n",
    "\n",
    "3. Kernel Selection: Choose an appropriate kernel function (e.g., linear, RBF, polynomial) based on the\n",
    "      nature of your data.\n",
    "\n",
    "4. Optimization: The SVM algorithm solves the optimization problem (either hard or soft margin) to find the\n",
    "      optimal weight vector $w$ and bias $b$. This involves finding the Lagrange multipliers $\\alpha_i$.\n",
    "\n",
    "5. Support Vectors Identification: During optimization, only the data points that lie on or within the\n",
    "      margin (i.e., the support vectors) will have non-zero $\\alpha_i$ values. These are the critical points\n",
    "      that define the hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b9c9a5",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc2e3e0",
   "metadata": {},
   "source": [
    "To classify a new data point $x_{new}$:\n",
    "\n",
    "\n",
    "1. Calculate the decision function:\n",
    "    $$f(x_{new}) = \\text{sgn}(w \\cdot x_{new} - b)$$\n",
    "    or, in terms of support vectors and kernel function:\n",
    "    $$f(x_{new}) = \\text{sgn}\\left(\\sum_{i \\in SV} \\alpha_i y_i K(x_i, x_{new}) + b\\right)$$\n",
    "    Where $SV$ denotes the set of support vectors.\n",
    "2. The sign of $f(x_{new})$ determines the predicted class label (e.g., +1 or -1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c304fd7",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3c4214",
   "metadata": {},
   "source": [
    "SVMs are powerful due to their ability to handle high-dimensional data, their strong theoretical\n",
    "  foundation (maximizing margin), and their flexibility with the kernel trick to model non-linear\n",
    "  relationships. The choice of kernel and the regularization parameter $C$ are crucial for optimal\n",
    "  performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
